\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{ch:ack}{{}{v}{Acknowledgments}{chapter*.1}{}}
\newlabel{ch:abstract}{{}{vi}{Abstract}{chapter*.2}{}}
\citation{martin2004}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{kanade1981recovery}
\citation{Koenderink1984}
\citation{lineCurvedObjects}
\citation{mild-sugihara}
\citation{mild-sugihara}
\citation{furukawa-PAMI10}
\citation{Snavely2006}
\citation{malik1989recovering}
\citation{kanade1981recovery}
\citation{hoiem-ROB}
\citation{furukawa-PAMI10}
\citation{qishan2014Occluding}
\citation{mild-sugihara}
\citation{gcontext-hoiem}
\citation{hedau2012}
\citation{schwing2012b}
\citation{Lifting3DManhattan}
\citation{lee2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Paper's Content}{1}{section.1.1}}
\citation{depthorder}
\citation{depthorder}
\citation{newcombe2011kinectfusion}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{Fouhey14c}
\citation{Fouhey14c}
\citation{Eigen2014}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \it  The edges in (a) are marked in (b) with the color code: red (occluding), green (planar), blue (convex), and yellow (concave). Planar edges are caused by different phenomena as marked. (c) shows the Kinect depth map (note the depth quantization artifacts).}}{2}{figure.1.1}}
\newlabel{fig:EdgeLabeling}{{1.1}{2}{\it The edges in (a) are marked in (b) with the color code: red (occluding), green (planar), blue (convex), and yellow (concave). Planar edges are caused by different phenomena as marked. (c) shows the Kinect depth map (note the depth quantization artifacts)}{figure.1.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RGB Image}}}{2}{figure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Edge Types}}}{2}{figure.1.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Depth Map}}}{2}{figure.1.1}}
\citation{3dFromSingleView}
\citation{Authors06}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Edges in scene understanding}{3}{section.1.2}}
\newlabel{sec:SECTION1NAME}{{1.2}{3}{Edges in scene understanding}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Depth estimation techniques}{3}{section.1.3}}
\newlabel{sec:SECTION2NAME}{{1.3}{3}{Depth estimation techniques}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Monocular}{3}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Stereo}{3}{subsection.1.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}3D scanner}{3}{subsection.1.3.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{4}{chapter.2}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:relatedWork}{{2}{4}{Related Work}{chapter.2}{}}
\citation{mild-sugihara}
\citation{martin2004}
\citation{martin2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Edge labeling using point cloud}{5}{chapter.3}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:edgeLabelKinect}{{3}{5}{Edge labeling using point cloud}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Method}{5}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Contour Labeling}{5}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \it  The edges in (a) are marked in (b) with the color code: red (occluding), green (planar), blue (convex), and yellow (concave). Planar edges are caused by different phenomena as marked. (c) shows the Kinect depth map (note the depth quantization artifacts).}}{6}{figure.3.1}}
\newlabel{fig:EdgeLabeling}{{3.1}{6}{\it The edges in (a) are marked in (b) with the color code: red (occluding), green (planar), blue (convex), and yellow (concave). Planar edges are caused by different phenomena as marked. (c) shows the Kinect depth map (note the depth quantization artifacts)}{figure.3.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RGB Image}}}{6}{figure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Edge Types}}}{6}{figure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Depth Map}}}{6}{figure.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \it  This figure summarizes the pipeline of our approach. It shows RGB and depth maps as input (1st image set), with Pb edge detection\nobreakspace  {}\cite  {martin2004} (2nd image). The classification and MRF outputs are shown in the last two images respectively. Color code: red (occ), green (pln), blue (cvx), yellow (ccv).}}{6}{figure.3.2}}
\newlabel{fig:pipeline}{{3.2}{6}{\it This figure summarizes the pipeline of our approach. It shows RGB and depth maps as input (1st image set), with Pb edge detection~\cite {martin2004} (2nd image). The classification and MRF outputs are shown in the last two images respectively. Color code: red (occ), green (pln), blue (cvx), yellow (ccv)}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Contour graph}{6}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \it  Contour segments are part of edge links that is bounded by two junctions as shown in (a). In (b), we show a graph where edge junctions are nodes and edge links are edges. In (c), we show a graph with contour segments $c_i$ as nodes and junctions lead to edges between nodes.}}{7}{figure.3.3}}
\newlabel{fig:graph_construction}{{3.3}{7}{\it Contour segments are part of edge links that is bounded by two junctions as shown in (a). In (b), we show a graph where edge junctions are nodes and edge links are edges. In (c), we show a graph with contour segments $c_i$ as nodes and junctions lead to edges between nodes}{figure.3.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Edge Links}}}{7}{figure.3.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Junction Graph}}}{7}{figure.3.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Contour Graph}}}{7}{figure.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Unary Potentials}{7}{subsection.3.2.2}}
\newlabel{unary}{{3.1}{7}{Unary Potentials}{equation.3.2.1}{}}
\newlabel{SC@1}{{3.2.2}{8}{Unary Potentials}{equation.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \it  Edge Pixel Neighborhood: The graph potentials of an edge pixel $b$ are defined based on a neighborhood consisting of four pixel on either side of the edge ($p_i$'s and $q_i$'s) on a line perpendicular to the gradient edge direction at $b$.}}{8}{figure.3.4}}
\newlabel{fig:edgeNeighbors}{{3.4}{8}{\SC@CAPtext }{figure.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \it  Elements of the feature vector used in pixel-wise edge classifier. Here, the indices $i$, $j$ and $k$ vary from 1 to 4.}}{8}{table.3.1}}
\newlabel{table:featureVector}{{3.1}{8}{\it Elements of the feature vector used in pixel-wise edge classifier. Here, the indices $i$, $j$ and $k$ vary from 1 to 4}{table.3.1}{}}
\citation{boykov2001fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Inference using graph cuts}{9}{subsection.3.2.3}}
\newlabel{eq:datasmooth}{{3.3}{9}{Inference using graph cuts}{equation.3.2.3}{}}
\citation{canny}
\citation{martin2004}
\citation{isola14crisp}
\citation{edgeDetectSurvey-Koschan}
\citation{Silberman:ECCV12}
\newlabel{eq:E}{{3.4}{10}{Inference using graph cuts}{equation.3.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Experiments}{10}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \it  Sample image from our dataset showing the RGB image, depth map and edge groundtruth. Colors: red (occ), green (pln), blue (cvx), yellow (ccv).}}{10}{figure.3.5}}
\newlabel{fig:dataset_depth_GT}{{3.5}{10}{\it Sample image from our dataset showing the RGB image, depth map and edge groundtruth. Colors: red (occ), green (pln), blue (cvx), yellow (ccv)}{figure.3.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {RGB}}}{10}{figure.3.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Depth Map}}}{10}{figure.3.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Ground truth}}}{10}{figure.3.5}}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Evaluation and Numerical Results}{11}{subsection.3.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces \it  Precision, Recall and F-measure for each edge type on our and NYU datasets. $1^{st}$ and $2^{nd}$ rows of each set gives the results of our approach and comparison with \nobreakspace  {}\cite  {gupta13Perceptual}. The $3^{rd}$ row in each set shows the results of our approach on NYU dataset.}}{11}{table.3.2}}
\newlabel{table:evaluation}{{3.2}{11}{\it Precision, Recall and F-measure for each edge type on our and NYU datasets. $1^{st}$ and $2^{nd}$ rows of each set gives the results of our approach and comparison with ~\cite {gupta13Perceptual}. The $3^{rd}$ row in each set shows the results of our approach on NYU dataset}{table.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces \it  Confusion matrix across the four classes. The numbers given are the average number of edge pixels per image.}}{11}{table.3.3}}
\newlabel{table:confusion}{{3.3}{11}{\it Confusion matrix across the four classes. The numbers given are the average number of edge pixels per image}{table.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces \it  Precision, recall and F-measure for each edge type without and with pairwise potentials.}}{11}{table.3.4}}
\newlabel{table:compare}{{3.4}{11}{\it Precision, recall and F-measure for each edge type without and with pairwise potentials}{table.3.4}{}}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\citation{gupta13Perceptual}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \it  NYU dataset results : Ground truths (above) and the corresponding results from our approach (below). Color code: red (occ), green (pln), blue (cvx), yellow (ccv).}}{13}{figure.3.6}}
\newlabel{fig:nyuResults}{{3.6}{13}{\it NYU dataset results : Ground truths (above) and the corresponding results from our approach (below). Color code: red (occ), green (pln), blue (cvx), yellow (ccv)}{figure.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces \it  Ground truths (above) and the corresponding results from our approach ($2^{nd}$ row) and Gupta {\em  et al.}\nobreakspace  {}\cite  {gupta13Perceptual} ($3^{rd}$ row). Color code: red (occ), green (pln), blue (cvx), yellow (ccv).}}{13}{figure.3.7}}
\newlabel{fig:comp2}{{3.7}{13}{\it Ground truths (above) and the corresponding results from our approach ($2^{nd}$ row) and Gupta {\em et al.}~\cite {gupta13Perceptual} ($3^{rd}$ row). Color code: red (occ), green (pln), blue (cvx), yellow (ccv)}{figure.3.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{13}{figure.3.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{13}{figure.3.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{13}{figure.3.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {}}}{13}{figure.3.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {}}}{13}{figure.3.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces \it  Ground truths (above) and the corresponding results from our approach ($2^{nd}$ row) and Gupta {\em  et al.}\nobreakspace  {}\cite  {gupta13Perceptual} ($3^{rd}$ row). Color code: red (occ), green (pln), blue (cvx), yellow (ccv).}}{14}{figure.3.8}}
\newlabel{fig:comp}{{3.8}{14}{\it Ground truths (above) and the corresponding results from our approach ($2^{nd}$ row) and Gupta {\em et al.}~\cite {gupta13Perceptual} ($3^{rd}$ row). Color code: red (occ), green (pln), blue (cvx), yellow (ccv)}{figure.3.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {}}}{14}{figure.3.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Edge labeling using Stereo}{15}{chapter.4}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:edgeLabelStereo}{{4}{15}{Edge labeling using Stereo}{chapter.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{16}{chapter.5}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:conc}{{5}{16}{Conclusion}{chapter.5}{}}
\newlabel{ch:relatedPubs}{{5}{17}{Related Publications}{chapter*.6}{}}
\bibstyle{latex8}
\bibdata{egbib}
\bibcite{boykov2001fast}{1}
\bibcite{canny}{2}
\bibcite{Eigen2014}{3}
\bibcite{Fouhey14c}{4}
\bibcite{furukawa-PAMI10}{5}
\bibcite{gupta13Perceptual}{6}
\bibcite{hedau2012}{7}
\bibcite{gcontext-hoiem}{8}
\bibcite{hoiem-ROB}{9}
\bibcite{isola14crisp}{10}
\bibcite{depthorder}{11}
\bibcite{kanade1981recovery}{12}
\bibcite{Koenderink1984}{13}
\bibcite{edgeDetectSurvey-Koschan}{14}
\bibcite{lee2010}{15}
\bibcite{lineCurvedObjects}{16}
\bibcite{malik1989recovering}{17}
\bibcite{martin2004}{18}
\bibcite{newcombe2011kinectfusion}{19}
\bibcite{Lifting3DManhattan}{20}
\@writefile{toc}{{\@tempskipb 3.0ex plus 1pt\relax }}
\@writefile{toc}{\contentsline {chapter}{{Bibliography}}{18}{chapter*.7}}
\bibcite{schwing2012b}{21}
\bibcite{qishan2014Occluding}{22}
\bibcite{Silberman:ECCV12}{23}
\bibcite{Snavely2006}{24}
\bibcite{mild-sugihara}{25}
